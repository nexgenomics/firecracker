
## How to set up a firecracker host


05Dec25: There is a lot of stuff in this file which captures the twisted journey
I took to get bridging to work securely with firecracker guests.
This is a rolling archive and a lot of this material is obsoleted by subsequent
work. A TODO is to collect all the known-good stuff at the top of the file.



This method keeps the main network interface out of the bridge
and uses masquerading so the guests all appear on the LAN with the
MAC and IP addresses of the main interface.
It depends on forwarding between the bridge and the hardware NIC.

It does NOT depend on enslaving the NIC to the bridge. *DO NOT* take
the advice that is out there to enslave the NIC to the bridge.



### The bridge

We put a /16 subnet on the bridge to enable a target of up to 1000 guests.

The best way to do this is via netplan. Add the following yaml under the
network keyword:
  bridges:
    fire0:
      parameters:
        stp: false
        forward-delay: 0
      addresses:
      - 10.0.0.1/16

The following also works:
  ip link add fire0 type bridge
  ip addr add 10.0.0.1/16 dev fire0
  ip link set fire0 up


### Nftables

It would be wonderful to use nftables instead of iptables, but we're
stuck because Docker uses legacy iptables, and we want to stay compatible
in the sense that a firecracker host can run docker containers with
network access.

According to lore, Docker/iptables can coexist with nftables but I never
figured out the right magic words, and life is too short.

So as of now, nftables is disabled. Since this isn't the default, say:

  # systemctl disable nftables.service




### Taps

Each guest VM is associated with a network tap. The taps do not take
IP addresses, and they are enslaved to the fire0 bridge.

We're hoping for a healthy maximum of 64 guests per host (that might
grow later), so we need to create at least that many taps.

We have a script here named create_taps.sh. Run this script under systemd
after the multi-user.target completes.

Each tap has a unique name in the form tapXXX, where XXX is a "slot number."
Slots are integers starting from zero.

The create_taps.sh script is dependent on the presence of the fire0 bridge,
which is created by netplan. So there's a little timing loop in create_taps.sh
that waits for fire0 to exist before it starts creating the taps.


### Filtering and forwarding setup (iptables)

This setup is done in create_taps.sh, after the taps are created. Figuring
all this out was cursed, so be appreciative of the effort.

What we want is for each guest VM to have an IP address in the bridge's subnet
(10.0.0.0/16), and for the guests to access the local network environment
by masquerading through the main NIC of the host machine.

We also want the guests to be inaccesible from the network and from each other.
We do want them to be able to make connections out to the network.

Here's how to set all this up in iptables (assuming that the hardware NIC is eno1):

  # modprobe br_netfilter
  # sysctl -w net.bridge.bridge-nf-call-iptables=1
  # sysctl -w net.bridge.bridge-nf-call-ip6tables=1
  # sysctl -w net.bridge.bridge-nf-call-arpables=1
  # iptables -t nat -A POSTROUTING -o eno1 -s 10.0.0.0/16 -j MASQUERADE
  # iptables -A FORWARD -i fire0 -o eno1 -j ACCEPT
  # iptables -A FORWARD -i eno1 -o fire0 -m state --state RELATED,ESTABLISHED -j ACCEPT

It's actually pretty simple, considering what it took to figure out.
The br_netfilter is subtle. It's not enabled by default, but modprobing it
is all it takes to enable it.

And what does it do? Very simple. It routes all intra-bridge traffic through iptables.

Since the default iptables policy for FORWARD is DROP, simply enabling br_netfilter
has the effect of blocking all traffic between the guest VMs, because the traffic
looks like move among the taps enslaved to the bridge.

Next, we masquerade all traffic from the bridge through the main NIC.

And we enable forwarding from the bridge (hence the taps) to and from the main NIC.


And this is most of what you need to know. Below is historical information that may
be of some interest.


-------------------------------------------------------------------


### Forwarding needed

sysctl -w net.ipv4.ip_forward=1
echo "net.ipv4.ip_forward=1" >> /etc/sysctl.conf


# replace eno1 with the real interface on the host
iptables -t nat -A POSTROUTING -o eno1 -s 10.1.1.0/16 -j MASQUERADE

# And make sure that forwarding is enabled on iptables. Something like:
iptables -A FORWARD -i fire0 -o eno1 -j ACCEPT
iptables -A FORWARD -i eno1 -o fire0 -m state --state RELATED,ESTABLISHED -j ACCEPT
# although that may already be set up by default policy on the machine,
# especially if docker was installed. Ie, -A FORWARD policy may already be ACCEPT.


Or even better, add the following to /etc/nftables.conf (the second stanza firewalls
the guests away from each other):

table ip for_firecracker {
	chain forward {
		type filter hook forward priority filter + 10; policy drop;
		iifname "fire0" oifname "eno1" accept
		iifname "eno1" oifname "fire0" accept
	}

	chain postrouting {
		type nat hook postrouting priority srcnat; policy accept;
		oifname "eno1" ip saddr 10.0.0.0/16 masquerade
	}
}
table bridge for_firecracker_br {
	chain forward {
		table filter hook forward priority 0; policy accept;
		iifname "tap*" oifname "tap*" drop
	}
}


This takes the place of all the iptables stuff.

Now the big problem with this nftables setup is invoking it on system startup.
Nftables.service does not default to enabled on Ubuntu, but we can't just enable it
because systemctl will run it after the sysinit.target. The fire0 bridge may not
yet have been created by netplan at that time (or something else is wrong), but
the forwarding rules just don't seem to "take."
It also doesn't work to move nftables.service to the multi-user.target.
The solution which does seem to work is to add a line to the bottom of the
script (below) that creates the tap interfaces for the firecracker guests.
That script does run under systemd, and it DOES wait for the bridge to be set up.
Also, there doesn't seem to be any problem with invoking systemctl start from within
a script that was invoked by systemd. So that's the answer.



95Dec25: More on the above: this approach interacts with docker in very strange ways.
The setup as described appears to work, but it BLOCKS Docker containers from reaching
the internet. Nothing I tried would solve that problem, so I just disabled docker.
Well, THAT breaks firecracker. So for now the answer is:
- disable nftables
- enable docker
- run create_taps.sh (which starts nftables) after the multi-user target in systemd.
That makes firecracker forwarding and masquerading work, but it blocks docker.

On further consideration, the easier path is to forego nftables and step back to iptables.
The following rules work and don't kill docker:
  iptables -t nat -A POSTROUTING -o eno1 -s 10.1.1.0/16 -j MASQUERADE
  iptables -A FORWARD -i fire0 -o eno1 -j ACCEPT
  iptables -A FORWARD -i eno1 -o fire0 -m state --state RELATED,ESTABLISHED -j ACCEPT



### Kvm

Since we want to run firecracker as an unprivileged user, ensure the unpriv
user is part of the kvm group:
```
sudo usermod -aG kvm francis
```
and then restart the shell.


### Taps

# create a tap for each guest. for guest #100:
ip tuntap add tap100 mode tap user francis
ip link set tap100 master fire0
ip link set tap100 up

Making the taps owned by an unpriv user allows firecracker to run unpriv! ðŸ˜Š


in each firecracker guest config (this example is for guest 100,
and each MAC must be unique. For convenience, map the ip address
to the MAC as shown below).

"network-interfaces":[
	{
		"iface_id":"eth0",
		"host_dev_name": "tap100",
		"guest_mac": "02:FC:00:00:00:64"
	}
]



Inside the guest (but this should be done by a startup script):

ip addr add 10.1.1.100/16 dev eth0
ip link set eth0 up
ip ro add default via 10.1.1.1




# Automatically install taps via systemd

/etc/systemd/system/firecracker_taps.service:

[Unit]
Description=Create persistent Firecracker TAP devices
After=network-online.target
Wants=network-online.target

[Service]
Type=oneshot
RemainAfterExit=yes
ExecStart=/home/francis/FIRECRACKER/create_taps.sh

[Install]
WantedBy=multi-user.target

(and systemctl enable it)
The create_taps script inserts a wait loop to make sure
the bridge exists before adding the taps. It also
starts nftables to make sure we have the right forwarding setup.

/home/francis/FIRECRACKER/create_taps.sh:
```
#!/bin/bash
# /usr/local/bin/create-firecracker-taps.sh
for try in {1..30}; do
    [[ -d /sys/class/net/fire0 ]] && break
    echo "Waiting for fire0 to appear ($try/30)..."
    sleep 1
done

[[ ! -d /sys/class/net/fire0 ]] && { echo "fire0 never appeared"; exit 1; }

for i in {100..199}; do
    ip tuntap add dev tap$i mode tap user francis 2>/dev/null || break
    ip link set tap$i master fire0 up 2>/dev/null || sleep 0.1
done

sleep 1
systemctl start nftables.service
```






Next, to firewall the guests away from each other, do this on the host
(or better, add it to /etc/nftables.conf, as shown above):

nft add table bridge filter
nft add chain bridge filter forward '{ type filter hook forward priority 0 ; }'
nft add rule bridge filter forward iifname "tap*" oifname "tap*" drop




For DHCP (Deprecated!):
on the host:
apt install dnsmasq

Edit /etc/dnsmasq.conf:
interface=fire0
bind-interfaces
dhcp-range=10.1.1.100,10.1.1.200,12h
# default gateway:
dhcp-option=3,10.1.1.1
# upstream DNS, if desired.
dhcp-option=6,8.8.8.8


in the guests:
apt install isc-dhcp-client
ip link set dev eth0 up
dhclient eth0




# Using iptables to block traffic from one guest to another.

05Dec25
To prevent traffic from crossing the bridge from one guest to another,
the only thing needed is:
  sudo modprobe br_netfilter
This has the effect of causing bridge traffic to pass through iptables, which
it otherwise DOES NOT, so it gets trapped by the default FORWARD DROP policy.

The following settings are also required to obtain this behavior, but they
appear to be defaults:
  sudo sysctl -w net.bridge.bridge-nf-call-iptables=1
  sudo sysctl -w net.bridge.bridge-nf-call-ip6tables=1
  sudo sysctl -w net.bridge.bridge-nf-call-arpables=1

None of this impacts docker, because docker sets up its own forwarding rules
that allow traffic from containers to the rest of the network.



